import torch

from .lr_scheduler import LRSchedulerWithWarmup


def build_optimizer(args, model):
    params = []

    print(f'Using {args.lr_factor} times learning rate for random init module ')
    
    for key, value in model.named_parameters():
        if not value.requires_grad:
            continue
        lr = args.lr
        weight_decay = args.weight_decay

        # è·¨æ¨¡æ€æ¨¡å— (cross_attn, cross_modal_transformer)
        if "cross" in key:
            # use large learning rate for random initialized cross modal module
            lr =  args.lr * args.lr_factor # default 5.0
        
        if "bias" in key:
            lr = args.lr * args.bias_lr_factor
            weight_decay = args.weight_decay_bias
        # Gate Projectionï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        if "gate_proj" in key:
            lr = args.lr * args.lr_factor
            # âœ… æ‰“å°ä¸åŒæ¨¡å—çš„ gate
            if "visual.transformer" in key:
                print(f"ğŸ”¥ Vision Gate: {key}, lr={lr:.2e}")
            elif "transformer.resblocks" in key and "cross" not in key:
                print(f"ğŸ”¥ Text Gate: {key}, lr={lr:.2e}")
            elif "cross" in key:
                print(f"ğŸ”¥ Cross-Modal Gate: {key}, lr={lr:.2e}")
            elif "id_gate" in key:
                print(f"ğŸ”¥ ID Branch Gate: {key}, lr={lr:.2e}")
        # åˆ†ç±»å™¨ã€MLMå¤´ã€IDç›¸å…³æ¨¡å—
        # if "classifier" in key or "mlm_head" in key or "text_id_" in key or "image_id_" in key or "fusion_" in key:
        #     lr = args.lr * args.lr_factor
        if ("classifier" in key or 
            "mlm_head" in key or 
            "text_id_" in key or 
            "image_id_" in key or 
            "id_pooling" in key or      # å¤šå±‚æ± åŒ–æ¨¡å—
            "id_query" in key or        # å•å±‚Queryï¼ˆæ–¹æ¡ˆ1/2ï¼‰
            "id_attention" in key or    # å…±äº«MHAï¼ˆæ–¹æ¡ˆ1/2ï¼‰
            "id_layernorm" in key or    # å…±äº«LNï¼ˆæ–¹æ¡ˆ1/2ï¼‰
            "shared_id_" in key or      # æ˜¾å¼å…±äº«å‘½å
            "fusion_" in key):          # ç‰¹å¾èåˆ
            lr = args.lr * args.lr_factor
        
        params += [{"params": [value], "lr": lr, "weight_decay": weight_decay}]

    if args.optimizer == "SGD":
        optimizer = torch.optim.SGD(
            params, lr=args.lr, momentum=args.momentum
        )
    elif args.optimizer == "Adam":
        optimizer = torch.optim.Adam(
            params,
            lr=args.lr,
            betas=(args.alpha, args.beta),
            eps=1e-3,
        )
    elif args.optimizer == "AdamW":
        optimizer = torch.optim.AdamW(
            params,
            lr=args.lr,
            betas=(args.alpha, args.beta),
            eps=1e-8,
        )
    else:
        NotImplementedError

    print("\n=== Learning Rate Assignment Summary ===")
    print(f"Base LR: {args.lr:.2e}")
    print(f"LR Factor: {args.lr_factor}")
    print(f"Bias LR Factor: {args.bias_lr_factor}")
    print(f"High LR: {args.lr * args.lr_factor:.2e}")

    print("\n--- ID Module Assignment (including ID gates) ---")
    for key, value in model.named_parameters():
        if not value.requires_grad:
            continue
        # æ˜¾ç¤ºæ‰€æœ‰IDç›¸å…³å‚æ•°ï¼ˆåŒ…æ‹¬é—¨æ§ï¼‰
        if ("text_id_" in key or "image_id_" in key or 
            "id_query" in key or "id_attention" in key or 
            "id_layernorm" in key):
            actual_lr = args.lr * args.bias_lr_factor if "bias" in key else args.lr * args.lr_factor
            gate_mark = "ğŸ”¥ [Gate]" if "gate_proj" in key else "âœ…"
            print(f"{gate_mark} LR={actual_lr:.2e}: {key}")

    print("\n--- CMT Gate Assignment ---")
    for key, value in model.named_parameters():
        if not value.requires_grad:
            continue
        # åªæ˜¾ç¤ºCMTçš„gate_projï¼ˆæ’é™¤IDé—¨æ§ï¼‰
        if "gate_proj" in key and "cross_modal_transformer" in key:
            actual_lr = args.lr * args.bias_lr_factor if "bias" in key else args.lr * args.lr_factor
            print(f"ğŸ”¥ LR={actual_lr:.2e}: {key}")

    print("=" * 50)

    return optimizer


def build_lr_scheduler(args, optimizer):
    return LRSchedulerWithWarmup(
        optimizer,
        milestones=args.milestones,
        gamma=args.gamma,
        warmup_factor=args.warmup_factor,
        warmup_epochs=args.warmup_epochs,
        warmup_method=args.warmup_method,
        total_epochs=args.num_epoch,
        mode=args.lrscheduler,
        target_lr=args.target_lr,
        power=args.power,
    )
